{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library\n****","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport copy\nimport math\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-12-08T06:48:57.118176Z","iopub.execute_input":"2022-12-08T06:48:57.119071Z","iopub.status.idle":"2022-12-08T06:48:58.335325Z","shell.execute_reply.started":"2022-12-08T06:48:57.118948Z","shell.execute_reply":"2022-12-08T06:48:58.334121Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Clean & analyze data\n****","metadata":{}},{"cell_type":"code","source":"heart_data = pd.read_csv(\"/kaggle/input/heart-failure-prediction/heart.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2022-12-08T06:48:58.337573Z","iopub.execute_input":"2022-12-08T06:48:58.338305Z","iopub.status.idle":"2022-12-08T06:48:58.363278Z","shell.execute_reply.started":"2022-12-08T06:48:58.338264Z","shell.execute_reply":"2022-12-08T06:48:58.361800Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"heart_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-08T07:03:23.704247Z","iopub.execute_input":"2022-12-08T07:03:23.704729Z","iopub.status.idle":"2022-12-08T07:03:23.715002Z","shell.execute_reply.started":"2022-12-08T07:03:23.704692Z","shell.execute_reply":"2022-12-08T07:03:23.713972Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(918, 12)"},"metadata":{}}]},{"cell_type":"code","source":"heart_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-08T07:03:25.756516Z","iopub.execute_input":"2022-12-08T07:03:25.757091Z","iopub.status.idle":"2022-12-08T07:03:25.783950Z","shell.execute_reply.started":"2022-12-08T07:03:25.757022Z","shell.execute_reply":"2022-12-08T07:03:25.782557Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n0   40   M           ATA        140          289          0     Normal    172   \n1   49   F           NAP        160          180          0     Normal    156   \n2   37   M           ATA        130          283          0         ST     98   \n3   48   F           ASY        138          214          0     Normal    108   \n4   54   M           NAP        150          195          0     Normal    122   \n\n  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n0              N      0.0       Up             0  \n1              N      1.0     Flat             1  \n2              N      0.0       Up             0  \n3              Y      1.5     Flat             1  \n4              N      0.0       Up             0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n      <th>HeartDisease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>140</td>\n      <td>289</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>172</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>F</td>\n      <td>NAP</td>\n      <td>160</td>\n      <td>180</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>156</td>\n      <td>N</td>\n      <td>1.0</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>130</td>\n      <td>283</td>\n      <td>0</td>\n      <td>ST</td>\n      <td>98</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48</td>\n      <td>F</td>\n      <td>ASY</td>\n      <td>138</td>\n      <td>214</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>108</td>\n      <td>Y</td>\n      <td>1.5</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54</td>\n      <td>M</td>\n      <td>NAP</td>\n      <td>150</td>\n      <td>195</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>122</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"heart_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-12-08T07:04:41.695956Z","iopub.execute_input":"2022-12-08T07:04:41.696386Z","iopub.status.idle":"2022-12-08T07:04:41.717735Z","shell.execute_reply.started":"2022-12-08T07:04:41.696356Z","shell.execute_reply":"2022-12-08T07:04:41.716718Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 918 entries, 0 to 917\nData columns (total 12 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Age             918 non-null    int64  \n 1   Sex             918 non-null    object \n 2   ChestPainType   918 non-null    object \n 3   RestingBP       918 non-null    int64  \n 4   Cholesterol     918 non-null    int64  \n 5   FastingBS       918 non-null    int64  \n 6   RestingECG      918 non-null    object \n 7   MaxHR           918 non-null    int64  \n 8   ExerciseAngina  918 non-null    object \n 9   Oldpeak         918 non-null    float64\n 10  ST_Slope        918 non-null    object \n 11  HeartDisease    918 non-null    int64  \ndtypes: float64(1), int64(6), object(5)\nmemory usage: 86.2+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Removing object column\n****\nfrom info above I will remove object type columns and only use integer and float column","metadata":{}},{"cell_type":"code","source":"del_col = []\nfor col in heart_data.columns:\n    if heart_data[col].dtype == float or heart_data[col].dtype == int:\n        pass\n    else:\n        del_col.append(col)\n    if heart_data[col].isnull().all():\n        del_col.append(col)\n        \nprint(del_col)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T07:45:38.850998Z","iopub.execute_input":"2022-12-08T07:45:38.851574Z","iopub.status.idle":"2022-12-08T07:45:38.866863Z","shell.execute_reply.started":"2022-12-08T07:45:38.851533Z","shell.execute_reply":"2022-12-08T07:45:38.865289Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n","output_type":"stream"}]},{"cell_type":"code","source":"clean_data = heart_data.drop(del_col,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T08:03:36.903852Z","iopub.execute_input":"2022-12-08T08:03:36.904322Z","iopub.status.idle":"2022-12-08T08:03:36.913651Z","shell.execute_reply.started":"2022-12-08T08:03:36.904285Z","shell.execute_reply":"2022-12-08T08:03:36.911960Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"clean_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-12-08T08:03:41.942637Z","iopub.execute_input":"2022-12-08T08:03:41.943828Z","iopub.status.idle":"2022-12-08T08:03:41.965342Z","shell.execute_reply.started":"2022-12-08T08:03:41.943764Z","shell.execute_reply":"2022-12-08T08:03:41.964102Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 918 entries, 0 to 917\nData columns (total 7 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   Age           918 non-null    int64  \n 1   RestingBP     918 non-null    int64  \n 2   Cholesterol   918 non-null    int64  \n 3   FastingBS     918 non-null    int64  \n 4   MaxHR         918 non-null    int64  \n 5   Oldpeak       918 non-null    float64\n 6   HeartDisease  918 non-null    int64  \ndtypes: float64(1), int64(6)\nmemory usage: 50.3 KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Separate \"Heart Disease\" Column\n****","metadata":{}},{"cell_type":"code","source":"X_data = clean_data.drop([\"HeartDisease\"],axis=1).values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-12-08T08:14:06.502677Z","iopub.execute_input":"2022-12-08T08:14:06.503136Z","iopub.status.idle":"2022-12-08T08:14:06.512163Z","shell.execute_reply.started":"2022-12-08T08:14:06.503098Z","shell.execute_reply":"2022-12-08T08:14:06.510481Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"Y_data = clean_data[\"HeartDisease\"].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-12-08T08:14:07.972004Z","iopub.execute_input":"2022-12-08T08:14:07.972419Z","iopub.status.idle":"2022-12-08T08:14:07.979557Z","shell.execute_reply.started":"2022-12-08T08:14:07.972388Z","shell.execute_reply":"2022-12-08T08:14:07.978068Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(X_data[:3])\nprint(Y_data[:3])","metadata":{"execution":{"iopub.status.busy":"2022-12-08T08:14:09.447359Z","iopub.execute_input":"2022-12-08T08:14:09.447744Z","iopub.status.idle":"2022-12-08T08:14:09.453438Z","shell.execute_reply.started":"2022-12-08T08:14:09.447714Z","shell.execute_reply":"2022-12-08T08:14:09.452018Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[[40.0, 140.0, 289.0, 0.0, 172.0, 0.0], [49.0, 160.0, 180.0, 0.0, 156.0, 1.0], [37.0, 130.0, 283.0, 0.0, 98.0, 0.0]]\n[0, 1, 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Feature Normalization\n****\nIn this model I am implementing z-score normalization, adjust your input values as shown in this formula:\n$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{1}$$ ","metadata":{}},{"cell_type":"code","source":"def zscore_normalize_features(X):\n    mu = np.mean(X, axis=0)\n    sigma = np.std(X, axis=0)\n    X_norm = (X - mu)/sigma\n    return (X_norm,sigma,mu)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T08:16:17.062861Z","iopub.execute_input":"2022-12-08T08:16:17.063268Z","iopub.status.idle":"2022-12-08T08:16:17.069312Z","shell.execute_reply.started":"2022-12-08T08:16:17.063237Z","shell.execute_reply":"2022-12-08T08:16:17.067854Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"X_norm, X_mu, X_sigma = zscore_normalize_features(X_data)\nprint(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\nprint(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_data,axis=0)}\")   \nprint(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-08T08:16:39.364394Z","iopub.execute_input":"2022-12-08T08:16:39.364830Z","iopub.status.idle":"2022-12-08T08:16:39.378942Z","shell.execute_reply.started":"2022-12-08T08:16:39.364793Z","shell.execute_reply":"2022-12-08T08:16:39.377638Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"X_mu = [  9.42747752  18.50406741 109.3245509    0.42281514  25.44646308\n   1.06598907], \nX_sigma = [ 53.51089325 132.39651416 198.79956427   0.23311547 136.80936819\n   0.88736383]\nPeak to Peak range by column in Raw        X:[ 49.  200.  603.    1.  142.    8.8]\nPeak to Peak range by column in Normalized X:[ 5.19757272 10.80843447  5.51568696  2.36509977  5.58034331  8.2552441 ]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(X_norm.shape)\nprint(X_norm[:2])","metadata":{"execution":{"iopub.status.busy":"2022-12-08T08:18:02.473346Z","iopub.execute_input":"2022-12-08T08:18:02.474633Z","iopub.status.idle":"2022-12-08T08:18:02.480921Z","shell.execute_reply.started":"2022-12-08T08:18:02.474573Z","shell.execute_reply":"2022-12-08T08:18:02.479517Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"(918, 6)\n[[-1.4331398   0.41090889  0.82507026 -0.55134134  1.38292822 -0.83243239]\n [-0.47848359  1.49175234 -0.17196105 -0.55134134  0.75415714  0.10566353]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Splitting data into train and test\n****\nin this notebook I split the data with ratio 2/3 for training data and 1/3 for test data.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(\n    X_norm, Y_data, test_size=0.33, random_state=4\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T08:19:51.284463Z","iopub.execute_input":"2022-12-08T08:19:51.284914Z","iopub.status.idle":"2022-12-08T08:19:51.294001Z","shell.execute_reply.started":"2022-12-08T08:19:51.284878Z","shell.execute_reply":"2022-12-08T08:19:51.292632Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Sigmoid function for logistic regression\n****\n\n$$g(z) = \\frac{1}{1+e^{-z}} \\tag{1} $$ ","metadata":{}},{"cell_type":"code","source":"def sigmoid(z):\n    g = 1/(1 + np.exp(-z))\n    return g","metadata":{"execution":{"iopub.status.busy":"2022-12-08T08:30:17.397744Z","iopub.execute_input":"2022-12-08T08:30:17.398236Z","iopub.status.idle":"2022-12-08T08:30:17.403805Z","shell.execute_reply.started":"2022-12-08T08:30:17.398197Z","shell.execute_reply":"2022-12-08T08:30:17.402602Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Cost function for logistic regression\n****\n$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n\nwhere\n* m is the number of training examples in the dataset\n\n\n* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is - \n\n    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n    \n    \n*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n\n*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n    * It might be helpful to first calculate an intermediate variable $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ where $n$ is the number of features, before calculating $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$","metadata":{}},{"cell_type":"code","source":"def compute_cost(X, y,w,b,lambda_ =1):\n    m, n = X.shape\n    total_cost = 0\n    for i in range(m):\n        z = np.dot(X[i],w) + b\n        f_x = sigmoid(z)\n        total_cost += (-y[i]*np.log(f_x)) - (1 - y[i])*np.log(1-f_x)\n    \n    total_cost = total_cost/m\n    \n    return total_cost","metadata":{"execution":{"iopub.status.busy":"2022-12-08T09:16:21.970223Z","iopub.execute_input":"2022-12-08T09:16:21.970624Z","iopub.status.idle":"2022-12-08T09:16:21.978512Z","shell.execute_reply.started":"2022-12-08T09:16:21.970593Z","shell.execute_reply":"2022-12-08T09:16:21.977298Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Descent for logistic regression\n****\n$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n\nwhere, parameters $b$, $w_j$ are all updated simultaniously\nb and w equation can be explained by below equation:\n$$\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n$$\n$$\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n$$","metadata":{}},{"cell_type":"code","source":"def compute_gradient(X, y, w, b, lambda_ = None):\n    m, n = X.shape\n    dj_dw = np.zeros(w.shape)\n    dj_db = 0.\n    \n    for i in range(m):\n        z_wb = 0\n        for j in range(n):\n            z_wb += X[i,j]*w[j]\n        z_wb += b\n        f_wb = sigmoid(z_wb)\n        \n        dj_db_i = f_wb - y[i]\n        dj_db += dj_db_i\n        \n        for j in range(n):\n            dj_dw_ij = dj_db_i*X[i][j]\n            dj_dw[j] += dj_dw_ij\n    \n    dj_dw = dj_dw/m\n    dj_db = dj_db/m\n    \n    return dj_db, dj_dw","metadata":{"execution":{"iopub.status.busy":"2022-12-08T09:26:41.648708Z","iopub.execute_input":"2022-12-08T09:26:41.649370Z","iopub.status.idle":"2022-12-08T09:26:41.659726Z","shell.execute_reply.started":"2022-12-08T09:26:41.649326Z","shell.execute_reply":"2022-12-08T09:26:41.658122Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_):\n    m = len(X)\n    \n    J_history = []\n    w_history = []\n    \n    for i in range(num_iters):\n        \n        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)\n        \n        w_in = w_in - alpha*dj_dw\n        b_in = b_in - alpha*dj_db\n        \n        if(i < 100000):\n            cost = cost_function(X, y, w_in, b_in, lambda_)\n            J_history.append(cost)\n        \n        if(i % math.ceil(num_iters/10) == 0 or i == (num_iters-1)):\n            w_history.append(w_in)\n            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n    \n    return w_in, b_in, J_history, w_history","metadata":{"execution":{"iopub.status.busy":"2022-12-08T09:26:43.693144Z","iopub.execute_input":"2022-12-08T09:26:43.693962Z","iopub.status.idle":"2022-12-08T09:26:43.703824Z","shell.execute_reply.started":"2022-12-08T09:26:43.693914Z","shell.execute_reply":"2022-12-08T09:26:43.702639Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Predict Function\n****","metadata":{}},{"cell_type":"code","source":"def predict(X, w, b):\n    m,n = X.shape\n    p = np.zeros(m)\n    \n    for i in range(m):\n        z_wb = 0\n        for j in range(n):\n            z_wb += X[i,j] * w[j]\n        z_wb += b\n        f_wb = sigmoid(z_wb)\n        p[i] = f_wb >= 0.5\n    \n    return p","metadata":{"execution":{"iopub.status.busy":"2022-12-08T09:26:45.446931Z","iopub.execute_input":"2022-12-08T09:26:45.447364Z","iopub.status.idle":"2022-12-08T09:26:45.454696Z","shell.execute_reply.started":"2022-12-08T09:26:45.447328Z","shell.execute_reply":"2022-12-08T09:26:45.453235Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Learning Parameters\n****","metadata":{}},{"cell_type":"code","source":"np.random.seed(1)\ninitial_w = np.random.rand(X_train.shape[1]) - 0.5\ninitial_b = 1.\nlambda_ = 0.01;\niterations = 10000\nalpha = 0.01\nprint(X_train[:5],initial_w)\nw, b, J_history,_ = gradient_descent(X_train, Y_train,initial_w, initial_b, \n                                   compute_cost, compute_gradient, \n                                   alpha, iterations, lambda_)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T09:56:50.668109Z","iopub.execute_input":"2022-12-08T09:56:50.669212Z","iopub.status.idle":"2022-12-08T09:58:29.902935Z","shell.execute_reply.started":"2022-12-08T09:56:50.669171Z","shell.execute_reply":"2022-12-08T09:58:29.901592Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"[[-0.16026485  0.30282455  0.22136323 -0.55134134  1.26503364 -0.83243239]\n [-1.96350436 -0.56185021 -0.06219613 -0.55134134  1.4615246  -0.83243239]\n [-0.26633776 -0.12951283 -0.18110812 -0.55134134 -1.44654163 -0.83243239]\n [ 0.2640268   2.03217406 -1.81843477 -0.55134134 -0.54268321  1.51280741]\n [-0.90277524 -1.21035628  0.59639336 -0.55134134 -0.18899948  0.29328271]] [-0.082978    0.22032449 -0.49988563 -0.19766743 -0.35324411 -0.40766141]\nIteration    0: Cost     0.77   \nIteration 1000: Cost     0.49   \nIteration 2000: Cost     0.49   \nIteration 3000: Cost     0.49   \nIteration 4000: Cost     0.49   \nIteration 5000: Cost     0.49   \nIteration 6000: Cost     0.49   \nIteration 7000: Cost     0.49   \nIteration 8000: Cost     0.49   \nIteration 9000: Cost     0.49   \nIteration 9999: Cost     0.49   \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation Normal Logistic Regression\n****","metadata":{}},{"cell_type":"code","source":"X_test\np = predict(X_test, w, b)\nprint('Train Accuracy: %f'%(np.mean(p == Y_test) * 100))","metadata":{"execution":{"iopub.status.busy":"2022-12-08T09:58:29.904804Z","iopub.execute_input":"2022-12-08T09:58:29.905298Z","iopub.status.idle":"2022-12-08T09:58:29.914523Z","shell.execute_reply.started":"2022-12-08T09:58:29.905263Z","shell.execute_reply":"2022-12-08T09:58:29.913060Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Train Accuracy: 75.907591\n","output_type":"stream"}]},{"cell_type":"code","source":"#normal logistic regression prediction\nY_test\nnormal_logistic = pd.DataFrame(Y_test, columns=['Heart Disease Test'])\nnormal_logistic[\"prediction\"] = p\nnormal_logistic\nnormal_logistic.to_csv(\"normal_logistic.csv\", index = None)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T09:50:33.224677Z","iopub.execute_input":"2022-12-08T09:50:33.225157Z","iopub.status.idle":"2022-12-08T09:50:33.239365Z","shell.execute_reply.started":"2022-12-08T09:50:33.225119Z","shell.execute_reply":"2022-12-08T09:50:33.237978Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Regularized Logistic Regression\n****\nIn this notebook I will use PolynomialFeatures from ****Sklearn**** for feature mapping, this step is necessary so we can fit our data better.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures","metadata":{"execution":{"iopub.status.busy":"2022-12-08T10:26:06.362406Z","iopub.execute_input":"2022-12-08T10:26:06.362830Z","iopub.status.idle":"2022-12-08T10:26:06.368774Z","shell.execute_reply.started":"2022-12-08T10:26:06.362795Z","shell.execute_reply":"2022-12-08T10:26:06.367520Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"poly = PolynomialFeatures(degree=2)\n#PolynomialFeatures(interaction_only=True) degree=2\n\nX_mapped = poly.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:10:17.971975Z","iopub.execute_input":"2022-12-08T11:10:17.973218Z","iopub.status.idle":"2022-12-08T11:10:17.980229Z","shell.execute_reply.started":"2022-12-08T11:10:17.973157Z","shell.execute_reply":"2022-12-08T11:10:17.979013Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"print(X_train[:1])\nprint(X_mapped[:1])","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:14:02.187386Z","iopub.execute_input":"2022-12-08T11:14:02.187877Z","iopub.status.idle":"2022-12-08T11:14:02.196742Z","shell.execute_reply.started":"2022-12-08T11:14:02.187829Z","shell.execute_reply":"2022-12-08T11:14:02.195345Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"[[-0.16026485  0.30282455  0.22136323 -0.55134134  1.26503364 -0.83243239]]\n[[ 1.         -0.16026485  0.30282455  0.22136323 -0.55134134  1.26503364\n  -0.83243239  0.02568482 -0.04853213 -0.03547674  0.08836064 -0.20274042\n   0.13340965  0.09170271  0.06703422 -0.16695969  0.38308324 -0.25208096\n   0.04900168 -0.1220467   0.28003193 -0.18426992  0.30397727 -0.69746534\n   0.45895439  1.60031011 -1.05305498  0.69294369]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Compute cost for Regularized Logistic Regression\n****\n$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$\n\nAs you can see from equation above there is difference in from normal compute cost, that is:\n$$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{2}$$","metadata":{}},{"cell_type":"code","source":"def compute_cost_reg(X, y, w, b, lambda_ = 1):\n    m,n = X.shape\n    \n    cost_without_reg = compute_cost(X,y,w,b)\n    reg_cost = 0.\n    #print(m,n)\n    #print(\"cost_without_reg\",cost_without_reg)\n    for i in range(n):\n        reg_cost += w[i]**2\n        \n    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost\n    \n    return total_cost","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:10:21.376193Z","iopub.execute_input":"2022-12-08T11:10:21.376620Z","iopub.status.idle":"2022-12-08T11:10:21.383517Z","shell.execute_reply.started":"2022-12-08T11:10:21.376586Z","shell.execute_reply":"2022-12-08T11:10:21.382123Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Descent for Regularized Logistic Regression\n****\n$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{1} $$\n\n$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$} \\tag{2}$$\n\n\nAs you can see,$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ is the same, the difference is the following term in $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, which is $$\\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$} \\tag{3} $$ \n\nSo we're going to calculate gradient descent plus equation (3) as code below","metadata":{}},{"cell_type":"code","source":"def compute_gradient_reg(X, y, w, b, lambda_ = 1):\n    m,n = X.shape\n    \n    dj_db, dj_dw = compute_gradient(X, y, w, b)\n    \n    for j in range(n):\n        dj_dw_j_reg = (lambda_/m)*w[j]\n        dj_dw[j] += dj_dw_j_reg\n    \n    return dj_db, dj_dw","metadata":{"execution":{"iopub.status.busy":"2022-12-08T09:51:12.061939Z","iopub.execute_input":"2022-12-08T09:51:12.062990Z","iopub.status.idle":"2022-12-08T09:51:12.069293Z","shell.execute_reply.started":"2022-12-08T09:51:12.062945Z","shell.execute_reply":"2022-12-08T09:51:12.068132Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Regularized Logistic Regression\n****","metadata":{}},{"cell_type":"code","source":"np.random.seed(1)\ninitial_w = np.random.rand(X_mapped.shape[1]) - 0.5\ninitial_b = 1.\nlambda_ = 0.01;\niterations = 10000\nalpha = 0.01\nw_reg, b_reg, J_history,_ = gradient_descent(X_mapped, Y_train,initial_w, initial_b, \n                                   compute_cost_reg, compute_gradient_reg, \n                                   alpha, iterations, lambda_)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:10:24.303872Z","iopub.execute_input":"2022-12-08T11:10:24.304326Z","iopub.status.idle":"2022-12-08T11:14:02.185358Z","shell.execute_reply.started":"2022-12-08T11:10:24.304289Z","shell.execute_reply":"2022-12-08T11:14:02.184093Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Iteration    0: Cost     1.09   \nIteration 1000: Cost     0.47   \nIteration 2000: Cost     0.46   \nIteration 3000: Cost     0.46   \nIteration 4000: Cost     0.46   \nIteration 5000: Cost     0.46   \nIteration 6000: Cost     0.46   \nIteration 7000: Cost     0.46   \nIteration 8000: Cost     0.46   \nIteration 9000: Cost     0.46   \nIteration 9999: Cost     0.46   \n","output_type":"stream"}]},{"cell_type":"markdown","source":"Result for PolynomialFeatures(interaction_only=True)","metadata":{}},{"cell_type":"code","source":"#PolynomialFeatures(interaction_only=True)\nX_test_mapped = poly.fit_transform(X_test)\np = predict(X_test_mapped, w_reg, b_reg)\nprint('Test Accuracy: %f'%(np.mean(p == Y_test) * 100))","metadata":{"execution":{"iopub.status.busy":"2022-12-08T10:30:15.380093Z","iopub.execute_input":"2022-12-08T10:30:15.380515Z","iopub.status.idle":"2022-12-08T10:30:15.393414Z","shell.execute_reply.started":"2022-12-08T10:30:15.380481Z","shell.execute_reply":"2022-12-08T10:30:15.392099Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Test Accuracy: 77.887789\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Result for PolynomialFeatures(degree=2)","metadata":{}},{"cell_type":"code","source":"X_test_mapped = poly.fit_transform(X_test)\np = predict(X_test_mapped, w_reg, b_reg)\nprint('Test Accuracy: %f'%(np.mean(p == Y_test) * 100))","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:18:01.089143Z","iopub.execute_input":"2022-12-08T11:18:01.089652Z","iopub.status.idle":"2022-12-08T11:18:01.110603Z","shell.execute_reply.started":"2022-12-08T11:18:01.089609Z","shell.execute_reply":"2022-12-08T11:18:01.109294Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Test Accuracy: 80.198020\n","output_type":"stream"}]},{"cell_type":"code","source":"#Regularized logistic regression prediction\n#Y_test\nregularized_logistic = pd.DataFrame(Y_test, columns=['Heart Disease Test'])\nregularized_logistic[\"prediction\"] = p\nregularized_logistic\nregularized_logistic.to_csv(\"regularized_logistic.csv\", index = None)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T11:18:10.645529Z","iopub.execute_input":"2022-12-08T11:18:10.645946Z","iopub.status.idle":"2022-12-08T11:18:10.658764Z","shell.execute_reply.started":"2022-12-08T11:18:10.645910Z","shell.execute_reply":"2022-12-08T11:18:10.657308Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# SkLearn Logistic Regression\n****\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(C=1000.0, random_state=0)\nlr_model.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T10:54:19.375073Z","iopub.execute_input":"2022-12-08T10:54:19.375487Z","iopub.status.idle":"2022-12-08T10:54:19.389755Z","shell.execute_reply.started":"2022-12-08T10:54:19.375457Z","shell.execute_reply":"2022-12-08T10:54:19.387927Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(C=1000.0, random_state=0)"},"metadata":{}}]},{"cell_type":"code","source":"y_prediction = lr_model.predict(X_test)\nprint(\"Prediction on test set:\", y_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-12-08T10:54:21.337940Z","iopub.execute_input":"2022-12-08T10:54:21.338366Z","iopub.status.idle":"2022-12-08T10:54:21.345717Z","shell.execute_reply.started":"2022-12-08T10:54:21.338335Z","shell.execute_reply":"2022-12-08T10:54:21.344432Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Prediction on test set: [1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1\n 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0\n 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1\n 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0\n 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1\n 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1\n 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1\n 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0\n 1 0 1 1 0 1 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Accuracy on training set:\", lr_model.score(X_test, Y_test))","metadata":{"execution":{"iopub.status.busy":"2022-12-08T10:54:24.337899Z","iopub.execute_input":"2022-12-08T10:54:24.338331Z","iopub.status.idle":"2022-12-08T10:54:24.345580Z","shell.execute_reply.started":"2022-12-08T10:54:24.338296Z","shell.execute_reply":"2022-12-08T10:54:24.343959Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Accuracy on training set: 0.759075907590759\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Test Accuracy: %f'%(np.mean(y_prediction == Y_test) * 100))","metadata":{"execution":{"iopub.status.busy":"2022-12-08T10:54:28.919243Z","iopub.execute_input":"2022-12-08T10:54:28.920008Z","iopub.status.idle":"2022-12-08T10:54:28.925452Z","shell.execute_reply.started":"2022-12-08T10:54:28.919969Z","shell.execute_reply":"2022-12-08T10:54:28.924334Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Test Accuracy: 75.907591\n","output_type":"stream"}]}]}